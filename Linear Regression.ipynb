{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_BzmTTmDUBu"
   },
   "source": [
    "# Homework 1 - Linear Regression\n",
    "\n",
    "## Dataset\n",
    "The dataset you will be using is the \"Bike Sharing\". \n",
    "\n",
    "There are two data files: \"BikeSharing_training.csv\" and \"BikeSharing_Xtest.csv\"<br/>\n",
    "Both files have the following fields (except cnt, which is not available in \"BikeSharing_Xtest.csv\")\n",
    "\n",
    "Features:\n",
    "- season: season (1: winter, 2: spring, 3: summer, 4: fall)\n",
    "- mnth: month (1 to 12)\n",
    "- hr: hour (0 to 23)\n",
    "- holiday: whether day is holiday or not\n",
    "- weekday: day of the week\n",
    "- workingday: 1 if day is neither weekend nor holiday, 0 otherwise\n",
    "+ weathersit:\n",
    "    - 1: Clear, Few clouds, Partly cloudy\n",
    "    - 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "    - 3: Light Snow, Light Rain + Thunderstorms + Scattered clouds, Light Rain + Scattered clouds\n",
    "    - 4: Heavy Rain + Ice Pellets + Thunderstorm + Mist, Snow + Fog\n",
    "- temp: Normalized temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min = -8, t_max = +39 (only in hourly scale)\n",
    "- atemp: Normalized feeling temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min = -16, t_max = +50 (only in hourly scale)\n",
    "- hum: Normalized humidity. The values are divided by 100 (max)\n",
    "- windspeed: Normalized wind speed. The values are divided by 67 (max)\n",
    "\n",
    "Target:\n",
    "- cnt: count of total rental bikes\n",
    "\n",
    "\n",
    "The training dataset, \"BikeSharing_training.csv\", contains 300 rows and 12 columns. This is the training set containing both the features and the target.<br/>\n",
    "The test dataset, \"BikeSharing_Xtest.csv\", contains 200 rows and 11 columns. This is the test set, which only contains the features.<br/>\n",
    "\n",
    "Your goal is to predict the number of total rental bikes (cnt) based on the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HvhEodPZDUBx"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3ZxPF2tDUB1"
   },
   "source": [
    "Load the training data and view the first 5 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "qJ3MgD96DUB1",
    "outputId": "f246c969-046e-4a99-9ab3-bed756e9ba29"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.4242</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.1940</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.4848</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.4179</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.5606</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.6364</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.1940</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.7424</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.1642</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   season  mnth  hr  holiday  weekday  workingday  weathersit  temp   atemp  \\\n",
       "0       1    12  16        0        5           1           1  0.42  0.4242   \n",
       "1       4    10   9        0        0           0           2  0.50  0.4848   \n",
       "2       3     9   1        0        0           0           2  0.62  0.5606   \n",
       "3       3     6  22        0        3           1           1  0.70  0.6364   \n",
       "4       3     7  12        0        1           1           2  0.80  0.7424   \n",
       "\n",
       "    hum  windspeed  cnt  \n",
       "0  0.47     0.1940  283  \n",
       "1  0.55     0.4179  330  \n",
       "2  0.88     0.0000   88  \n",
       "3  0.42     0.1940  183  \n",
       "4  0.52     0.1642  314  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "training_data = pd.read_csv('BikeSharing_training.csv')\n",
    "\n",
    "# Show the first 5 lines\n",
    "training_data.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OWDB32zDUB4"
   },
   "source": [
    "## Data Exploration\n",
    "We can plot a histogram of the dataframe for the features \"temp\", \"atemp\", \"hum\", and \"windspeed\" to understand the distributions of the continuous values.<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "id": "HEL-h8hRDUB4",
    "outputId": "89193ff9-b9a0-4f00-c4d3-ee76fde3a9c5",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### WRITE CODE TO OBTAIN AND DISPLAY HISTOGRAMS ###\n",
    "hists = training_data.hist(column = ['temp', 'atemp', 'hum', 'windspeed'], figsize = (6,6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXl1UzKvDUB7"
   },
   "source": [
    "##### Q1. What can you infer from the histograms? <br/>\n",
    "Ans- \n",
    "\n",
    "**temp:** The temperatures are somewhat concentrated at the lower end of the temperature distribution.\n",
    "\n",
    "**atemp:** These temperatures are also somewhat concentrated at the lower end of the distribution. We expect this since it is directly related to the distribtution of temp.\n",
    "\n",
    "**hum:** The humidities are somwhat evenly distributed around the mean, though there are a few gaps such as at 0.50.\n",
    "\n",
    "**windspeed:** The wind speeds are heavily concentrated at the lower end of the distribution. We can infer from this histogram that the wind speed rarely ever gets above a normalized value of 0.3.\n",
    "\n",
    "From all these histograms collectively, we can also see that there aren't many (if any) outliers in general in these features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-L6BqEuFDUB7"
   },
   "source": [
    "Compute the correlation matrix to get an understanding of the correlation between cnt and the other features.<br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "u4yZQ0PVDUB7",
    "outputId": "d01b5618-7781-4c6b-aaad-328e25e4bf0f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>season</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.795990</td>\n",
       "      <td>-0.046306</td>\n",
       "      <td>-0.057180</td>\n",
       "      <td>-0.049248</td>\n",
       "      <td>0.087355</td>\n",
       "      <td>0.097812</td>\n",
       "      <td>0.403317</td>\n",
       "      <td>0.406946</td>\n",
       "      <td>0.155086</td>\n",
       "      <td>-0.110663</td>\n",
       "      <td>0.225385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mnth</th>\n",
       "      <td>0.795990</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000608</td>\n",
       "      <td>-0.012655</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>0.045749</td>\n",
       "      <td>0.098688</td>\n",
       "      <td>0.270690</td>\n",
       "      <td>0.265519</td>\n",
       "      <td>0.116613</td>\n",
       "      <td>-0.005633</td>\n",
       "      <td>0.144026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr</th>\n",
       "      <td>-0.046306</td>\n",
       "      <td>0.000608</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.104970</td>\n",
       "      <td>0.029337</td>\n",
       "      <td>-0.083801</td>\n",
       "      <td>-0.117088</td>\n",
       "      <td>0.049154</td>\n",
       "      <td>0.042834</td>\n",
       "      <td>-0.375612</td>\n",
       "      <td>0.162829</td>\n",
       "      <td>0.387503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>holiday</th>\n",
       "      <td>-0.057180</td>\n",
       "      <td>-0.012655</td>\n",
       "      <td>0.104970</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.065359</td>\n",
       "      <td>-0.216498</td>\n",
       "      <td>-0.059273</td>\n",
       "      <td>0.023122</td>\n",
       "      <td>0.020936</td>\n",
       "      <td>-0.064204</td>\n",
       "      <td>-0.043506</td>\n",
       "      <td>0.018659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday</th>\n",
       "      <td>-0.049248</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>0.029337</td>\n",
       "      <td>-0.065359</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.096844</td>\n",
       "      <td>-0.088959</td>\n",
       "      <td>-0.057797</td>\n",
       "      <td>-0.071621</td>\n",
       "      <td>-0.098027</td>\n",
       "      <td>0.073404</td>\n",
       "      <td>0.067401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>workingday</th>\n",
       "      <td>0.087355</td>\n",
       "      <td>0.045749</td>\n",
       "      <td>-0.083801</td>\n",
       "      <td>-0.216498</td>\n",
       "      <td>0.096844</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.131374</td>\n",
       "      <td>-0.007330</td>\n",
       "      <td>-0.001266</td>\n",
       "      <td>0.168470</td>\n",
       "      <td>-0.000534</td>\n",
       "      <td>-0.072353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weathersit</th>\n",
       "      <td>0.097812</td>\n",
       "      <td>0.098688</td>\n",
       "      <td>-0.117088</td>\n",
       "      <td>-0.059273</td>\n",
       "      <td>-0.088959</td>\n",
       "      <td>0.131374</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.014367</td>\n",
       "      <td>-0.018496</td>\n",
       "      <td>0.403589</td>\n",
       "      <td>-0.047412</td>\n",
       "      <td>-0.167125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temp</th>\n",
       "      <td>0.403317</td>\n",
       "      <td>0.270690</td>\n",
       "      <td>0.049154</td>\n",
       "      <td>0.023122</td>\n",
       "      <td>-0.057797</td>\n",
       "      <td>-0.007330</td>\n",
       "      <td>-0.014367</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992783</td>\n",
       "      <td>-0.037122</td>\n",
       "      <td>-0.096506</td>\n",
       "      <td>0.435036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>atemp</th>\n",
       "      <td>0.406946</td>\n",
       "      <td>0.265519</td>\n",
       "      <td>0.042834</td>\n",
       "      <td>0.020936</td>\n",
       "      <td>-0.071621</td>\n",
       "      <td>-0.001266</td>\n",
       "      <td>-0.018496</td>\n",
       "      <td>0.992783</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.017675</td>\n",
       "      <td>-0.126852</td>\n",
       "      <td>0.430083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hum</th>\n",
       "      <td>0.155086</td>\n",
       "      <td>0.116613</td>\n",
       "      <td>-0.375612</td>\n",
       "      <td>-0.064204</td>\n",
       "      <td>-0.098027</td>\n",
       "      <td>0.168470</td>\n",
       "      <td>0.403589</td>\n",
       "      <td>-0.037122</td>\n",
       "      <td>-0.017675</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.228720</td>\n",
       "      <td>-0.321508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>windspeed</th>\n",
       "      <td>-0.110663</td>\n",
       "      <td>-0.005633</td>\n",
       "      <td>0.162829</td>\n",
       "      <td>-0.043506</td>\n",
       "      <td>0.073404</td>\n",
       "      <td>-0.000534</td>\n",
       "      <td>-0.047412</td>\n",
       "      <td>-0.096506</td>\n",
       "      <td>-0.126852</td>\n",
       "      <td>-0.228720</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.034753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnt</th>\n",
       "      <td>0.225385</td>\n",
       "      <td>0.144026</td>\n",
       "      <td>0.387503</td>\n",
       "      <td>0.018659</td>\n",
       "      <td>0.067401</td>\n",
       "      <td>-0.072353</td>\n",
       "      <td>-0.167125</td>\n",
       "      <td>0.435036</td>\n",
       "      <td>0.430083</td>\n",
       "      <td>-0.321508</td>\n",
       "      <td>0.034753</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              season      mnth        hr   holiday   weekday  workingday  \\\n",
       "season      1.000000  0.795990 -0.046306 -0.057180 -0.049248    0.087355   \n",
       "mnth        0.795990  1.000000  0.000608 -0.012655  0.001790    0.045749   \n",
       "hr         -0.046306  0.000608  1.000000  0.104970  0.029337   -0.083801   \n",
       "holiday    -0.057180 -0.012655  0.104970  1.000000 -0.065359   -0.216498   \n",
       "weekday    -0.049248  0.001790  0.029337 -0.065359  1.000000    0.096844   \n",
       "workingday  0.087355  0.045749 -0.083801 -0.216498  0.096844    1.000000   \n",
       "weathersit  0.097812  0.098688 -0.117088 -0.059273 -0.088959    0.131374   \n",
       "temp        0.403317  0.270690  0.049154  0.023122 -0.057797   -0.007330   \n",
       "atemp       0.406946  0.265519  0.042834  0.020936 -0.071621   -0.001266   \n",
       "hum         0.155086  0.116613 -0.375612 -0.064204 -0.098027    0.168470   \n",
       "windspeed  -0.110663 -0.005633  0.162829 -0.043506  0.073404   -0.000534   \n",
       "cnt         0.225385  0.144026  0.387503  0.018659  0.067401   -0.072353   \n",
       "\n",
       "            weathersit      temp     atemp       hum  windspeed       cnt  \n",
       "season        0.097812  0.403317  0.406946  0.155086  -0.110663  0.225385  \n",
       "mnth          0.098688  0.270690  0.265519  0.116613  -0.005633  0.144026  \n",
       "hr           -0.117088  0.049154  0.042834 -0.375612   0.162829  0.387503  \n",
       "holiday      -0.059273  0.023122  0.020936 -0.064204  -0.043506  0.018659  \n",
       "weekday      -0.088959 -0.057797 -0.071621 -0.098027   0.073404  0.067401  \n",
       "workingday    0.131374 -0.007330 -0.001266  0.168470  -0.000534 -0.072353  \n",
       "weathersit    1.000000 -0.014367 -0.018496  0.403589  -0.047412 -0.167125  \n",
       "temp         -0.014367  1.000000  0.992783 -0.037122  -0.096506  0.435036  \n",
       "atemp        -0.018496  0.992783  1.000000 -0.017675  -0.126852  0.430083  \n",
       "hum           0.403589 -0.037122 -0.017675  1.000000  -0.228720 -0.321508  \n",
       "windspeed    -0.047412 -0.096506 -0.126852 -0.228720   1.000000  0.034753  \n",
       "cnt          -0.167125  0.435036  0.430083 -0.321508   0.034753  1.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3F54Xq_0DUB-"
   },
   "source": [
    "##### Answer the following questions:<br/>\n",
    "\n",
    "##### Q2. Why is the diagonal made up of 1's in the correlation matrix?<br/>\n",
    "Ans - The diagonal measures the correlation between the variable and itself, so it will always be 1.\n",
    "\n",
    "##### Q3. Why is the matrix symmetric along diagonal?<br/>\n",
    "Ans - The correlation between x and y is always equal to the correlation between y and x.\n",
    "\n",
    "##### Q4. Looking at the correlation matrix, if you had to choose one predictor for a simple linear regression model with cnt as the outcome, which one would you choose and why? <br/>\n",
    "Ans - I would choose temp since, out of all the variables available to us, it has the highest correlation with cnt and thus would give the highest r-squared value in a simple linear regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-M8K5WrDUB_"
   },
   "source": [
    "### Standardization of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phZDGqD3DUB_"
   },
   "source": [
    "Feature standardization makes the values of each feature in the data have zero-mean and unit-variance. This method is widely used for normalization in many machine learning algorithms. The general method of calculation is to determine the mean and standard deviation for each feature, then subtract the mean from each feature, then divide the values of each feature by its standard deviation:\n",
    "\n",
    "$x'$ = ($x$ - $\\bar{x}$)/$\\sigma$ \n",
    "\n",
    "where $x$ is the original feature vector,\n",
    "$\\bar{x}$ is the mean of the feature vector and\n",
    "$\\sigma$ is its standard deviation.\n",
    "\n",
    "This is also called Z-score Normalization. \n",
    "\n",
    "Perform Z-score Normalization on \"temp\", \"atemp\", \"hum\", and \"windspeed.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "YrankYyNDUB_",
    "outputId": "bd367283-9d23-4af0-c8a3-b1bdf84b1c4a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.407573</td>\n",
       "      <td>-0.303588</td>\n",
       "      <td>-0.790860</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.006557</td>\n",
       "      <td>0.044723</td>\n",
       "      <td>-0.359580</td>\n",
       "      <td>1.832666</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.627751</td>\n",
       "      <td>0.480398</td>\n",
       "      <td>1.419451</td>\n",
       "      <td>-1.586921</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.041881</td>\n",
       "      <td>0.916074</td>\n",
       "      <td>-1.060410</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.559543</td>\n",
       "      <td>1.525330</td>\n",
       "      <td>-0.521310</td>\n",
       "      <td>-0.243307</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.235832</td>\n",
       "      <td>-1.174364</td>\n",
       "      <td>1.042081</td>\n",
       "      <td>-0.731819</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.213622</td>\n",
       "      <td>0.219453</td>\n",
       "      <td>1.149901</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.821702</td>\n",
       "      <td>-0.912844</td>\n",
       "      <td>0.233430</td>\n",
       "      <td>0.489052</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.731284</td>\n",
       "      <td>0.741919</td>\n",
       "      <td>0.179520</td>\n",
       "      <td>-1.586921</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.511105</td>\n",
       "      <td>-0.390378</td>\n",
       "      <td>-0.844770</td>\n",
       "      <td>2.199255</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     season  mnth  hr  holiday  weekday  workingday  weathersit      temp  \\\n",
       "0         1    12  16        0        5           1           1 -0.407573   \n",
       "1         4    10   9        0        0           0           2  0.006557   \n",
       "2         3     9   1        0        0           0           2  0.627751   \n",
       "3         3     6  22        0        3           1           1  1.041881   \n",
       "4         3     7  12        0        1           1           2  1.559543   \n",
       "..      ...   ...  ..      ...      ...         ...         ...       ...   \n",
       "295       1     1  23        0        2           1           1 -1.235832   \n",
       "296       2     5  22        0        4           1           1  0.213622   \n",
       "297       2     5   6        0        4           1           1 -0.821702   \n",
       "298       3     7   7        0        6           0           1  0.731284   \n",
       "299       2     4  15        0        2           1           1 -0.511105   \n",
       "\n",
       "        atemp       hum  windspeed  cnt  \n",
       "0   -0.303588 -0.790860   0.000540  283  \n",
       "1    0.044723 -0.359580   1.832666  330  \n",
       "2    0.480398  1.419451  -1.586921   88  \n",
       "3    0.916074 -1.060410   0.000540  183  \n",
       "4    1.525330 -0.521310  -0.243307  314  \n",
       "..        ...       ...        ...  ...  \n",
       "295 -1.174364  1.042081  -0.731819   41  \n",
       "296  0.219453  1.149901   0.000540  172  \n",
       "297 -0.912844  0.233430   0.489052   89  \n",
       "298  0.741919  0.179520  -1.586921   45  \n",
       "299 -0.390378 -0.844770   2.199255   68  \n",
       "\n",
       "[300 rows x 12 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# initialize scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# columns to normalize\n",
    "cols_norm = ['temp', 'atemp', 'hum', 'windspeed']\n",
    "# initialize new dataframe to pass into scaler\n",
    "to_normalize = training_data[cols_norm]\n",
    "# fit scaler to data\n",
    "scaler.fit(to_normalize)\n",
    "# store normalized features in variable \"normalized\"\n",
    "normalized = scaler.transform(to_normalize)\n",
    "# reassign columns to their normalized forms\n",
    "training_data[cols_norm] = normalized\n",
    "\n",
    "# display\n",
    "training_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9Gp4uGgDUCC"
   },
   "source": [
    "##### Q5. What are the advantages and disadvantages of using Z-score Normalization?<br/>\n",
    "Ans-\n",
    "\n",
    "**Advantages:** Unlike min/max normalization, Z-score normalization handles outliers very well. Additionally, it accounts for both mean and variance.\n",
    "\n",
    "\n",
    "**Disadvantages:** Unlike min/max normalization, Z-score normalization does not ensure that features are on the exact same scale. It also assumes a normal distribution, and this is not always valid.\n",
    "\n",
    "##### Q6. In this dataset, do you need to use the Z-score Normalization? Explain.<br/>\n",
    "Ans- It doesn't seem to make too much sense to use Z-score normalization on temp, atemp, windspeed, and hum since they were all already normalized. As for whether or not Z-score is the specific normalization technique that is optimal, I think it could make sense for min-max normalization to be used (if it weren't already done) since there don't appear to be any significant outliers based on the hisograms above. In terms of the entire dataset, the scale of the different variables is different (e.g. month and hour can get up to 12 and 24, respectively), but this is not a problem since we will one-hot encode these variables (see below).\n",
    "\n",
    "Overall, I don't think doing Z-score normalization on this dataset is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmQudD7jDUCC"
   },
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "np1bNy8ZDUCD"
   },
   "source": [
    "\"temp\", \"atemp\", \"hum\" and \"windspeed\" are continuous values whereas the others contain discrete values. E.g. \"mnth\" can only take on the integers from 1 to 12. We need to perform one-hot encoding on discrete values for it to be processed in the model. One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction.\n",
    "\n",
    "Perform one-hot encoding on all the categorical features and print the shape of your encoded array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RRa9aJY2VqYP",
    "outputId": "e8747391-6fb1-4750-9e8b-4208d864c72e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 52)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# we need to one-hot encode season, mnth, hr,weekday, and weathersit\n",
    "\n",
    "# create new dataframe with only season\n",
    "season_encode = pd.DataFrame(training_data['season'])\n",
    "#initialize encoder\n",
    "season_enc = OneHotEncoder(drop = 'first')\n",
    "# one-hot encode\n",
    "season_df = pd.DataFrame(season_enc.fit_transform(season_encode).toarray())\n",
    "# add prefix to column names\n",
    "season_df = season_df.add_prefix('season_')\n",
    "\n",
    "# repeat for other columns\n",
    "\n",
    "# mnth\n",
    "mnth_encode = pd.DataFrame(training_data['mnth'])\n",
    "mnth_enc = OneHotEncoder(drop = 'first') \n",
    "mnth_df = pd.DataFrame(mnth_enc.fit_transform(mnth_encode).toarray())\n",
    "mnth_df = mnth_df.add_prefix('mnth_')\n",
    "\n",
    "# hr\n",
    "hr_encode = pd.DataFrame(training_data['hr'])\n",
    "hr_enc = OneHotEncoder(drop = 'first') \n",
    "hr_df = pd.DataFrame(hr_enc.fit_transform(hr_encode).toarray())\n",
    "hr_df = hr_df.add_prefix('hr_')\n",
    "\n",
    "# weekday\n",
    "weekday_encode = pd.DataFrame(training_data['weekday'])\n",
    "weekday_enc = OneHotEncoder(drop = 'first') \n",
    "weekday_df = pd.DataFrame(weekday_enc.fit_transform(weekday_encode).toarray())\n",
    "weekday_df = weekday_df.add_prefix('weekday_')\n",
    "\n",
    "# weathersit\n",
    "weathersit_encode = pd.DataFrame(training_data['weathersit'])\n",
    "weathersit_enc = OneHotEncoder(drop = 'first') \n",
    "weathersit_df = pd.DataFrame(weathersit_enc.fit_transform(weathersit_encode).toarray())\n",
    "weathersit_df = weathersit_df.add_prefix('weathersit_')\n",
    "\n",
    "# create dataframe without the variables that we one-hot encoded\n",
    "training_to_conc = training_data.drop(columns = ['season', 'mnth', 'hr', 'weekday', 'weathersit'])\n",
    "# list of dataframes to concatenate\n",
    "to_conc = [season_df, mnth_df, hr_df, weekday_df, weathersit_df, training_to_conc]\n",
    "# concatenating dataframes\n",
    "training_data_encoded = pd.concat(to_conc, axis = 1)\n",
    "\n",
    "# Print the shape of your encoded X\n",
    "training_data_encoded.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "Nqq3-5HvyrPR",
    "outputId": "de9d2cdf-cb91-4a17-ad21-fc685906e174"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season_0</th>\n",
       "      <th>season_1</th>\n",
       "      <th>season_2</th>\n",
       "      <th>mnth_0</th>\n",
       "      <th>mnth_1</th>\n",
       "      <th>mnth_2</th>\n",
       "      <th>mnth_3</th>\n",
       "      <th>mnth_4</th>\n",
       "      <th>mnth_5</th>\n",
       "      <th>mnth_6</th>\n",
       "      <th>...</th>\n",
       "      <th>weekday_5</th>\n",
       "      <th>weathersit_0</th>\n",
       "      <th>weathersit_1</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.407573</td>\n",
       "      <td>-0.303588</td>\n",
       "      <td>-0.790860</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006557</td>\n",
       "      <td>0.044723</td>\n",
       "      <td>-0.359580</td>\n",
       "      <td>1.832666</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.627751</td>\n",
       "      <td>0.480398</td>\n",
       "      <td>1.419451</td>\n",
       "      <td>-1.586921</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.041881</td>\n",
       "      <td>0.916074</td>\n",
       "      <td>-1.060410</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.559543</td>\n",
       "      <td>1.525330</td>\n",
       "      <td>-0.521310</td>\n",
       "      <td>-0.243307</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.235832</td>\n",
       "      <td>-1.174364</td>\n",
       "      <td>1.042081</td>\n",
       "      <td>-0.731819</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.213622</td>\n",
       "      <td>0.219453</td>\n",
       "      <td>1.149901</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.821702</td>\n",
       "      <td>-0.912844</td>\n",
       "      <td>0.233430</td>\n",
       "      <td>0.489052</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.731284</td>\n",
       "      <td>0.741919</td>\n",
       "      <td>0.179520</td>\n",
       "      <td>-1.586921</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.511105</td>\n",
       "      <td>-0.390378</td>\n",
       "      <td>-0.844770</td>\n",
       "      <td>2.199255</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     season_0  season_1  season_2  mnth_0  mnth_1  mnth_2  mnth_3  mnth_4  \\\n",
       "0         0.0       0.0       0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1         0.0       0.0       1.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2         0.0       1.0       0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3         0.0       1.0       0.0     0.0     0.0     0.0     0.0     1.0   \n",
       "4         0.0       1.0       0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "..        ...       ...       ...     ...     ...     ...     ...     ...   \n",
       "295       0.0       0.0       0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "296       1.0       0.0       0.0     0.0     0.0     0.0     1.0     0.0   \n",
       "297       1.0       0.0       0.0     0.0     0.0     0.0     1.0     0.0   \n",
       "298       0.0       1.0       0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "299       1.0       0.0       0.0     0.0     0.0     1.0     0.0     0.0   \n",
       "\n",
       "     mnth_5  mnth_6  ...  weekday_5  weathersit_0  weathersit_1  holiday  \\\n",
       "0       0.0     0.0  ...        0.0           0.0           0.0        0   \n",
       "1       0.0     0.0  ...        0.0           1.0           0.0        0   \n",
       "2       0.0     0.0  ...        0.0           1.0           0.0        0   \n",
       "3       0.0     0.0  ...        0.0           0.0           0.0        0   \n",
       "4       1.0     0.0  ...        0.0           1.0           0.0        0   \n",
       "..      ...     ...  ...        ...           ...           ...      ...   \n",
       "295     0.0     0.0  ...        0.0           0.0           0.0        0   \n",
       "296     0.0     0.0  ...        0.0           0.0           0.0        0   \n",
       "297     0.0     0.0  ...        0.0           0.0           0.0        0   \n",
       "298     1.0     0.0  ...        1.0           0.0           0.0        0   \n",
       "299     0.0     0.0  ...        0.0           0.0           0.0        0   \n",
       "\n",
       "     workingday      temp     atemp       hum  windspeed  cnt  \n",
       "0             1 -0.407573 -0.303588 -0.790860   0.000540  283  \n",
       "1             0  0.006557  0.044723 -0.359580   1.832666  330  \n",
       "2             0  0.627751  0.480398  1.419451  -1.586921   88  \n",
       "3             1  1.041881  0.916074 -1.060410   0.000540  183  \n",
       "4             1  1.559543  1.525330 -0.521310  -0.243307  314  \n",
       "..          ...       ...       ...       ...        ...  ...  \n",
       "295           1 -1.235832 -1.174364  1.042081  -0.731819   41  \n",
       "296           1  0.213622  0.219453  1.149901   0.000540  172  \n",
       "297           1 -0.821702 -0.912844  0.233430   0.489052   89  \n",
       "298           0  0.731284  0.741919  0.179520  -1.586921   45  \n",
       "299           1 -0.511105 -0.390378 -0.844770   2.199255   68  \n",
       "\n",
       "[300 rows x 52 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display encoded data\n",
    "training_data_encoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F98z-ogFDUCG"
   },
   "source": [
    "##### Q7. What are the advantages and disadvantages of using One-hot encoding?<br/>\n",
    "Ans-\n",
    "\n",
    "**Advantages:** One-hot encoding allows categorical variables to be represented in a model-friendly format that doesn't establish a numerical hierarchy among these categorical variables. For example, we wouldn't want to store each weekday as a value from 0-6 since that would suggest that some days are greater than or less than others.\n",
    "\n",
    "**Disadvantages:** One-hot encoding leads to the creation of a new column for every unique value in the initial categorical column, which often leads to dataframes with a very large amount of columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eu7KyvueDUCG"
   },
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "In the big data era, it is highly unlikely that we are interested in the effect of a single variable on another. To simultaneously account for the effects of multiple variables, we use multiple regression (which accounts for the covariances between predictors).\n",
    "\n",
    "While the algorithmic solution to multiple regression exists, it is easier to conceptualize in terms of linear algebra. The optimal $\\hat{\\beta}$ vector that minimizes the residual sum of squares is:\n",
    "\n",
    "$\\hat{\\beta} = (X^TX)^{-1}X^Ty $\n",
    "\n",
    "\n",
    "Perform multiple linear regression on the training dataset, where the outcome is \"cnt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1vJfMcAcDUCJ",
    "outputId": "2bb7101d-c28d-447a-81b4-fd692265bcf2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11285.732948075856"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Bulding and fitting the Linear Regression model \n",
    "X_df = training_data_encoded.drop(columns=['cnt'])\n",
    "Y_df = training_data_encoded[['cnt']]\n",
    "\n",
    "# convert to numpy arrays\n",
    "X = X_df.to_numpy()\n",
    "Y = Y_df.to_numpy()\n",
    "\n",
    "# initialize and fit regression\n",
    "lr = LinearRegression()\n",
    "regression = lr.fit(X, Y)\n",
    "\n",
    "# Evaluating the Linear Regression model by computing MSE on training set\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "Y_pred = regression.predict(X)\n",
    "mean_squared_error(Y_pred, Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qOmp51CDUCL"
   },
   "source": [
    "###### Q8. Print the value of bhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QoNJfJDn6JiC",
    "outputId": "53fcf345-fdd4-496f-9916-4a422d828b52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  26.71501198   -7.97495888   60.79123577  -33.19421721  -11.9598258\n",
      "   -21.73959075  -19.5062793   -31.52374803  -46.36317914  -54.08820774\n",
      "   -49.92398051  -23.37760105   -7.3439333   -65.36570268  -30.66975815\n",
      "    10.21113337  -12.51568885    9.50068885   -2.10631173   36.30800919\n",
      "   158.25938267  351.36237752  176.38395473   82.7259382   149.769285\n",
      "   204.93537706  202.91809514  152.24990438  181.34027065  226.670072\n",
      "   402.70355909  345.22213814  295.51123972  173.40463121  107.84383604\n",
      "   101.51101705   31.33268676   33.19610277   15.01867362    2.04462891\n",
      "    20.75345973   18.22205292   24.42212125   -3.58338732 -106.47127914\n",
      "    88.14484055    1.09007741  157.46885809  -88.7493333    -9.61226171\n",
      "   -12.90022449]]\n"
     ]
    }
   ],
   "source": [
    "print(regression.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ld8szmSJDUCN"
   },
   "source": [
    "###### Q9. Is there a problem of multicolinearity? Explain what you can do<br/>\n",
    "Ans- \n",
    "We attempted to avoid collinearity by dropping one of the columns for each of the variables that we one-hot encoded. However, there are still some variables (e.g. temp and atemp) that are highly correlated and thus we can drop one of these when finding the optimal subset of features to use in our regression in order to minimize multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BilLJMSJDUCO"
   },
   "source": [
    "### Goodness of fit\n",
    "\n",
    "A model can always make predictions. But it is important to determine how good the model is.\n",
    "How do we know that our model captures the data well? When evaluating model fit, a good metric is $R^2$, which corresponds to the amount of variance explained by the model. The formula for $R^2$ is the following:\n",
    "\n",
    "$R^2$ = $1 - \\dfrac{RSS}{TSS}$<br/>\n",
    "where:<br/>\n",
    "$RSS = \\Sigma(y - \\hat{y})^2$<br/>\n",
    "$TSS = \\Sigma(y - \\bar{y})^2$<br/>\n",
    "\n",
    "$R^2$ is also one metric for comparing models against each other. It is intuitive to say that the model that explains more variation in the data is a better fit than one that explains less variation. \n",
    "\n",
    "Fill in the code for calculation of R2 score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "EUWa9ritDUCO"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def get_r2_score(x_cols, y_cols):\n",
    "\n",
    "    # create the proper dataframe\n",
    "    X_df = training_data_encoded[x_cols]\n",
    "    Y_df = training_data_encoded[y_cols]\n",
    "\n",
    "    # convert to numpy arrays\n",
    "    X = X_df.to_numpy()\n",
    "    Y = Y_df.to_numpy()\n",
    "\n",
    "    # initialize and fit regression\n",
    "    lr = LinearRegression()\n",
    "    regression = lr.fit(X, Y)\n",
    "    Y_pred = lr.predict(X)\n",
    "    \n",
    "    return r2_score(Y, Y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zz77WAJgDUCR"
   },
   "source": [
    "#### R2 score for model with \"temp\" as predictor and \"cnt\" as outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pGGP70DaDUCS",
    "outputId": "886da4ac-6ba2-4f44-9e05-33014cdacad7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18925663564676132"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print R2 score\n",
    "get_r2_score(x_cols = ['temp'], y_cols = ['cnt'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1790ynsADUCV"
   },
   "source": [
    "#### R2 score for model with \"temp\", \"hum\" as predictors and \"cnt\" as outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vJse-YGXDUCV",
    "outputId": "050df28d-b19b-4f83-e3d6-7883b4f6f0d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2826295103178843"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print R2 score\n",
    "get_r2_score(x_cols = ['temp', 'hum'], y_cols = ['cnt'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PTi03bhDUCY"
   },
   "source": [
    "#### R2 score for model with  \"temp\", \"atemp\", \"hum\" as predictors and \"cnt\" as outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yC1TuLEXDUCY",
    "outputId": "6496bd79-36e4-4a4f-c3bf-5df450422e05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28380026219213783"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print R2 score\n",
    "get_r2_score(x_cols = ['temp', 'atemp', 'hum'], y_cols = ['cnt'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMpPLRpqDUCb"
   },
   "source": [
    "You can see $R^2$ is always going up as we keep adding features.\n",
    "\n",
    "This is one drawback of only using $R^2$ to evaluate your model. Adding predictors seems to always improve the predictive ability of your model, though it may not be true.\n",
    "\n",
    "That is to say, we are not necessarily interested in making a perfect prediciton of our training data. If we were, we would always use all of the predictors available. Rather, we would like to make a perfect prediction of our test data. In this case, adding all the predictors may not be a good idea due to the trade-off between bias and variance. Thus, we are interested in the most predictive features, in the hopes that we can create a simpler model that performs well in the future.\n",
    "\n",
    "This is why we consider another metric, Adjusted R2.\n",
    "The adjusted R-squared increases only if the new term improves the model more than would be expected by chance.\n",
    "\n",
    "\n",
    "$AdjustedR^2$ = $1 - \\dfrac{(1-R^2)(n-1)}{(n-k-1)}$<br/>\n",
    "where:<br/>\n",
    "n = number of samples<br/>\n",
    "k = number of features\n",
    "\n",
    "Fill in the code for calculation of adjusted R2 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDIGy9mvDUCb"
   },
   "source": [
    "#### Adjusted R2 score for model with \"temp\" as predictor and \"cnt\" as outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OM4Sov98DUCc",
    "outputId": "c3928342-c081-4523-92b2-237feb5a6c03"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18653602033013972"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_adjusted_r2(x_cols, y_cols):\n",
    "    \n",
    "    # compute r2, n, and k\n",
    "    r2 = get_r2_score(x_cols = x_cols, y_cols = y_cols)\n",
    "    n = training_data_encoded.shape[0]\n",
    "    k = len(x_cols)\n",
    "    \n",
    "    # compute adjusted r2\n",
    "    tmp1 = (1 - r2) * (n - 1)\n",
    "    tmp2 = (n - k - 1)\n",
    "    return (1 - tmp1/tmp2)\n",
    "\n",
    "# Print Adjusted R2 score\n",
    "get_adjusted_r2(x_cols = ['temp'], y_cols = ['cnt'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWjr84lxDUCe"
   },
   "source": [
    "#### Adjusted R2 score for model with \"temp\", \"hum\" as predictors and \"cnt\" as outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4oq_Knd1DUCe",
    "outputId": "47dd18f3-891f-4e1a-81ef-d1393cfcfd37"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2777987326095872"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print Adjusted R2 score\n",
    "get_adjusted_r2(x_cols = ['temp', 'hum'], y_cols = ['cnt'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJ-_KgPFDUCj"
   },
   "source": [
    "#### Adjusted R2 score for model with  \"temp\", \"atemp\", \"hum\" as predictors and \"cnt\" as outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "697lAhoLDUCk",
    "outputId": "7984ca29-4885-4a1a-a7fc-feebb84ed4e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27654148106570675"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print Adjusted R2 score\n",
    "get_adjusted_r2(x_cols = ['temp', 'atemp', 'hum'], y_cols = ['cnt'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91B2cmFkDUCm"
   },
   "source": [
    "### K-fold Cross-Validation\n",
    "\n",
    "However, adjusted R2 is not enough to help us achieve the best model. A more robust method is k-fold cross-validation.\n",
    "\n",
    "* Randomly split dataset into K equal-sized subsets, or folds\n",
    "* Treat each fold as validation set (train on all but K'th fold and test on K'th fold only)\n",
    "\n",
    "* The overall error is then the mean error over all K models.\n",
    "* Most common are 5- or 10-fold cross-validation\n",
    "\n",
    "Implement a 5-fold cross-validation by yourselves to find the best model in terms of Mean Square Error (MSE)\n",
    "\n",
    "**Do not use sklearn.model_selection.cross_val_score or other built-in cross-validaiton functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LJQn4rS1DUCm",
    "outputId": "92fb04b5-20d0-4003-abed-f0778f73eecf"
   },
   "outputs": [],
   "source": [
    "# Design a function to implement 5-fold cross-validation. \n",
    "# The input: training features X, training target y.\n",
    "# The output: the average of MSE over the 5 folds.\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# reinitialize training set\n",
    "train = pd.read_csv(\"BikeSharing_training.csv\")\n",
    "\n",
    "# seed for reproducibility\n",
    "import random\n",
    "random.seed(11)\n",
    "\n",
    "# define cross-validation function\n",
    "def cross_val_mse(X, y):\n",
    "    \n",
    "    # initialize list of mse values\n",
    "    mses = []\n",
    "    # generate and shuffle random indices\n",
    "    ins = [i for i in range(300)]\n",
    "    random.shuffle(ins)\n",
    "    # grab indices tat we will use to generate folds\n",
    "    folds = [ins[i::5] for i in range(5)]\n",
    "    \n",
    "    for idxs in folds:\n",
    "        # generate folds\n",
    "        x_test=X.loc[idxs,:]\n",
    "        x_train=X.loc[~X.index.isin(idxs)]\n",
    "        y_test=y.loc[idxs]\n",
    "        y_train=y.loc[~y.index.isin(idxs)] \n",
    "        \n",
    "        # initialize and fit regression, predict\n",
    "        reg = LinearRegression().fit(x_train,y_train)\n",
    "        y_pred = reg.predict(x_test)\n",
    "        # account for counts below the minimum value\n",
    "        y_pred[y_pred < 1] = 1\n",
    "        # track mse values\n",
    "        mses.append(mean_squared_error(y_test, y_pred))\n",
    "        \n",
    "    return(sum(mses) / len(mses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LJQn4rS1DUCm",
    "outputId": "92fb04b5-20d0-4003-abed-f0778f73eecf"
   },
   "outputs": [],
   "source": [
    "# define function to format data based on selected features\n",
    "def encoder(features):\n",
    "    \n",
    "    # store and format continuous and categorical features\n",
    "    num_features = list(filter(lambda x: x in [\"temp\", \"atemp\", \"hum\", \"windspeed\"], features))\n",
    "    l = list(filter(lambda x: x not in [\"temp\", \"atemp\", \"hum\", \"windspeed\"], features))\n",
    "    \n",
    "    # initialize and fit one-hot encoder\n",
    "    enc = OneHotEncoder(handle_unknown = 'error', drop = 'first')\n",
    "    enc.fit(train.loc[:,l])\n",
    "    \n",
    "    # grab selected features, format encoded data\n",
    "    c_names = enc.get_feature_names(l)\n",
    "    train_encoded = enc.transform(train.loc[:,l]).toarray()\n",
    "    x_coded = pd.DataFrame(train_encoded, columns = c_names)\n",
    "    x_coded = pd.concat([x_coded,train.loc[:,num_features]], axis = 1)\n",
    "    \n",
    "    return x_coded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LJQn4rS1DUCm",
    "outputId": "92fb04b5-20d0-4003-abed-f0778f73eecf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13639.841820337042\n",
      "['season, hr, holiday, workingday, weathersit, temp, hum']\n"
     ]
    }
   ],
   "source": [
    "# Using your above k-fold cross validation function, find the best combination of features (lowest averaged MSE)\n",
    "from itertools import combinations\n",
    "\n",
    "# initialize list of potential features to use\n",
    "features = ['season', 'mnth', 'hr', 'holiday', 'weekday', 'workingday',\n",
    "       'weathersit', 'temp', 'atemp', 'hum', 'windspeed']\n",
    "combs = []\n",
    "\n",
    "# check all combinations of 3 or more features\n",
    "for L in range(3, len(features) + 1):\n",
    "    for subset in combinations(features, L):\n",
    "        combs.append(list(subset))\n",
    "\n",
    "# initialize dictionary of scores\n",
    "scores = {}\n",
    "# generate score and store for every combination of variables\n",
    "for comb in combs:\n",
    "    x = encoder(train[comb])\n",
    "    y = train['cnt']\n",
    "    score = cross_val_mse(x,y)\n",
    "    scores[\", \".join(comb)] = score\n",
    "\n",
    "# output best average score and corresponding features\n",
    "temp = min(scores.values()) \n",
    "res = [key for key in scores if scores[key] == temp] \n",
    "\n",
    "print(temp)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4DYHrhPTDUCo",
    "outputId": "d0b844a3-db3b-47f8-9bb5-c6d7fb559feb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['season', ' hr', ' holiday', ' workingday', ' weathersit', ' temp', ' hum']\n"
     ]
    }
   ],
   "source": [
    "# Print the best features \n",
    "print(res[0].split(\",\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PV8KeR1IDUCp"
   },
   "source": [
    "### Test your model\n",
    "Now, apply your best model to predict the target values from the test feature set \"BikeSharing_Xtest.csv\". We will grade this part based on your prediction error.\n",
    "\n",
    "Hint: Please be careful on standardization and one-hot encoding (if you use). The test set should be consistent with the training set in terms of any transformation.\n",
    "\n",
    "Hint2: You may want to modify the previous steps to make the transformation of the test set consistent with the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eicsy-XamWrb",
    "outputId": "bf436134-86ec-43c0-fcd8-6ad7e2739fce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300.49392011 177.92519499 419.4245133  376.87581612 432.33478919\n",
      " 134.40766073   1.         227.29630823 141.6972331   60.74263354\n",
      "  57.51514226 118.19460404 479.95842406 109.20426385  95.4573173\n",
      " 149.89903955 332.5074636  309.27877677 348.11195515 137.63364679\n",
      "   7.519902   302.84181295  29.41889079 141.18664005 221.28865247\n",
      "   1.          52.47995762 338.58983318 242.39255676 232.42317836\n",
      " 249.2839011  442.83434101   1.           1.          34.48697207\n",
      "   1.         154.17333548 208.55010222 325.01243759  90.20345094\n",
      " 378.37738466 504.40616911  52.43498657 445.31617847 240.30092067\n",
      " 382.90420482 156.36469792 327.19117646 511.09953239 279.4685704\n",
      " 320.0344613  289.22711213 287.46134368  69.76713241 229.54382389\n",
      "   1.         341.8361081  328.45210118   1.           1.\n",
      " 213.10540623 410.48962248 100.04329691 280.5764834  158.45211763\n",
      " 211.92714015  33.07452937 317.03058302 216.69072755  78.72171277\n",
      " 625.76728866 298.73007649 146.49587916 498.88170183 484.43608862\n",
      " 446.68799491   1.          42.38146308 242.87583456 255.2798349\n",
      "   1.         183.39043611  19.59660006 212.56172507 128.59847231\n",
      "   1.         350.44637872  24.04078874 181.73503067 178.15307091\n",
      " 105.40926789  24.79316492 234.42315717  13.6240616  223.08653088\n",
      " 481.16050136 249.34004162 275.94890131 285.281797   393.11637649\n",
      "   1.          93.76521708 132.96521548   1.         171.81275242\n",
      " 368.04830884  78.13931112 523.2343203  278.1855129  154.31841434\n",
      "  98.39203072 101.88704313 512.35985071 233.2595602  150.85460589\n",
      " 139.73706078 493.30246554 229.4279623   42.72810988 153.53971397\n",
      "  28.31468859 304.42210312 285.19413666   1.         201.84862397\n",
      "  80.52622698 199.43448101 121.7649263   69.47409244 251.68908067\n",
      " 226.11904519  84.85040666 149.06346676 221.15655172 425.99364083\n",
      " 220.16630917 337.89989083  96.41929393  88.56615459 124.3253872\n",
      " 365.86295499 336.79951344 222.8009764   72.85883076 342.93643218\n",
      " 420.31545702 209.7266033    1.         335.9539025  167.42580388\n",
      " 295.09578299 200.39428221 220.93634427 108.53665994 232.42814254\n",
      "  21.71292868 251.22643863 276.12573635 185.75504475 332.38767025\n",
      "  59.51922677 199.31623292 190.98484283 201.62787323 245.72765541\n",
      " 313.66102746  82.45001782 505.5263047  355.26322516  90.12673992\n",
      " 235.03802258 554.72553455 218.94394298   1.          42.65864306\n",
      " 149.14800524 302.13617516 290.69742123  17.69005419 195.76640284\n",
      " 182.13401493   1.          72.63753067 242.19945791 374.12252713\n",
      " 128.44284017   7.69499835 153.34417739 164.42237485 595.92220503\n",
      "  85.32266857 215.84838039 262.02439148 349.62099452 193.31182043\n",
      " 421.38300477 110.49832566  10.28868438 193.81289343 214.90918621]\n"
     ]
    }
   ],
   "source": [
    "# define new function (for test set) to format data based on selected features\n",
    "def f_encoder(features):\n",
    "    \n",
    "    # store and format continuous and categorical features\n",
    "    num_features = list(filter(lambda x: x in [\"temp\", \"atemp\", \"hum\", \"windspeed\"], features))\n",
    "    l = list(filter(lambda x: x not in [\"temp\", \"atemp\", \"hum\", \"windspeed\"], features))\n",
    "    \n",
    "    # initialize and fit one-hot encoder\n",
    "    enc = OneHotEncoder(handle_unknown = 'error', drop = 'first')\n",
    "    enc.fit(final_x.loc[:,l])\n",
    "    \n",
    "    # grab selected features, format encoded data\n",
    "    c_names = enc.get_feature_names(l)\n",
    "    train_encoded = enc.transform(final_x.loc[:,l]).toarray()\n",
    "    x_coded = pd.DataFrame(train_encoded, columns = c_names)\n",
    "    x_coded = pd.concat([x_coded,final_x.loc[:,num_features]], axis = 1)\n",
    "    \n",
    "    return x_coded\n",
    "\n",
    "# read in test set\n",
    "final_x = pd.read_csv('BikeSharing_Xtest.csv')\n",
    "# format test set\n",
    "final_x_coded = f_encoder(['season', 'hr', 'holiday', 'workingday', 'weathersit', 'temp', 'hum'])\n",
    "\n",
    "# format training set including only the optimal features\n",
    "x_train = encoder(['season', 'hr', 'holiday', 'workingday', 'weathersit', 'temp', 'hum'])\n",
    "# initialize training labels\n",
    "y_train = train['cnt']\n",
    "\n",
    "# fit regression\n",
    "reg = LinearRegression().fit(x_train,y_train)\n",
    "\n",
    "# predict and output prediction\n",
    "y_pred = reg.predict(final_x_coded)\n",
    "y_pred[y_pred < 1] = 1\n",
    "\n",
    "# Output your prediction on test set as y_pred. It should be a 200 x 1 vector.\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape # checks out\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Exercise1_Team10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
